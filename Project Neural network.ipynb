{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matrix_util import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.client.session.Session object at 0x1a2f24da90>\n"
     ]
    }
   ],
   "source": [
    "print(str(sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3618927442451699071\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing CSV file containing image labels and corresponding image file names\n",
    "\n",
    "path_to_labels=\"/Users/muhammadwaliji/Desktop/facial_expressions-master/data/legend.csv\"\n",
    "\n",
    "data=pd.read_csv(path_to_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.id</th>\n",
       "      <th>image</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628</td>\n",
       "      <td>facial-expressions_2868588k.jpg</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628</td>\n",
       "      <td>facial-expressions_2868585k.jpg</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>628</td>\n",
       "      <td>facial-expressions_2868584k.jpg</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>628</td>\n",
       "      <td>facial-expressions_2868582k.jpg</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dwdii</td>\n",
       "      <td>Aaron_Eckhart_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user.id                            image   emotion\n",
       "0     628  facial-expressions_2868588k.jpg     anger\n",
       "1     628  facial-expressions_2868585k.jpg  surprise\n",
       "2     628  facial-expressions_2868584k.jpg   disgust\n",
       "3     628  facial-expressions_2868582k.jpg      fear\n",
       "4   dwdii           Aaron_Eckhart_0001.jpg   neutral"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing all the values in the emotion column\n",
    "\n",
    "data['emotion'] = [str(i).lower() for i in data[\"emotion\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-labelling all the columns labelled sad with sadness for consistency\n",
    "\n",
    "data['emotion']=data['emotion'].replace(to_replace='sadness', value='sad', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'surprise',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'neutral',\n",
       " 'happiness',\n",
       " 'sad',\n",
       " 'contempt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=list(data['emotion'].unique())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 4 emotions of choice i.e. anger, neutral, happiness, sad\n",
    "\n",
    "restricted_to_specific_emo=data.loc[data.emotion.isin([\"anger\",\"neutral\",\"happiness\",\"sad\"])].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'neutral', 'happiness', 'sad']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels we are going to use\n",
    "\n",
    "using_labels=list(restricted_to_specific_emo['emotion'].unique())\n",
    "using_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE OPEN CV2 to generate array for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the column name \"image\" from the dataframe labelled data and converting to list\n",
    "\n",
    "img_file_names=list(restricted_to_specific_emo[\"image\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the array value of each file\n",
    "\n",
    "grayscale_image_array=[]\n",
    "\n",
    "for i in range(len(img_file_names)):\n",
    "    img_path = f\"/Users/muhammadwaliji/Desktop/facial_expressions-master/images/{img_file_names[i]}\"\n",
    "    img = cv2.imread(img_path,0)\n",
    "    grayscale_image_array.append(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>num_images</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_rows</th>\n",
       "      <th>num_cols</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <th>350</th>\n",
       "      <td>12823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <th>73</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <th>80</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   num_images\n",
       "num_rows num_cols            \n",
       "350      350            12823\n",
       "37       27                 3\n",
       "91       73                 3\n",
       "99       80                 2\n",
       "73       58                 2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of images for each image size\n",
    "\n",
    "(\n",
    "    pd.DataFrame.from_records(\n",
    "        [img.shape for img in grayscale_image_array],\n",
    "        columns=['num_rows', 'num_cols'])\n",
    "    .groupby(['num_rows', 'num_cols'])\n",
    "    .size()\n",
    "    .rename('num_images')\n",
    "    .to_frame()\n",
    "    .sort_values(by=\"num_images\", ascending=False)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the array values of all the images of size 350 by 350 and their corresponding emotion values and\n",
    "# appending to new lists\n",
    "\n",
    "\n",
    "grayscale_image_array_only_350by350=[]\n",
    "emotions_only_350by350=[]\n",
    "\n",
    "\n",
    "for i in range(len(grayscale_image_array)):\n",
    "    img = grayscale_image_array[i]\n",
    "    emotion = restricted_to_specific_emo.emotion[i]\n",
    "    \n",
    "    if img.shape == (350, 350):\n",
    "        grayscale_image_array_only_350by350.append(img)\n",
    "        emotions_only_350by350.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12823\n",
      "12823\n"
     ]
    }
   ],
   "source": [
    "# Checking the size of the new lists\n",
    "\n",
    "print(len(grayscale_image_array_only_350by350))\n",
    "print(len(emotions_only_350by350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the orientation of the image in multiple ways to create a more robust data set which will then be used \n",
    "# to train and test the model\n",
    "\n",
    "images_reflected_horizontally=[]\n",
    "\n",
    "images_reflected_vertically=[]\n",
    "\n",
    "images_rotated_right=[]\n",
    "\n",
    "images_rotated_left=[]\n",
    "\n",
    "images_rotated_180=[]\n",
    "\n",
    "\n",
    "for i in range(len(grayscale_image_array_only_350by350)):\n",
    "    \n",
    "    # grabs images and assigns to img\n",
    "    img=grayscale_image_array_only_350by350[i]\n",
    "    \n",
    "    #reflects the image horizontally and appends to images_reflected_horizontally list\n",
    "    images_reflected_horizontally.append(reflect_horizontally(img))\n",
    "    \n",
    "    #reflects the image vertically and appends to images_reflected_vertically list\n",
    "    images_reflected_vertically.append(reflect_vertically(img))\n",
    "    \n",
    "    #rotates the image right and appends to images_rotated_right list\n",
    "    images_rotated_right.append(rotate_right(img))\n",
    "    \n",
    "    #rotates the image left and appends to images_rotated_left list\n",
    "    images_rotated_left.append(rotate_left(img))\n",
    "    \n",
    "    #rotates the image 180 degrees and appends to images_rotated_180 list\n",
    "    images_rotated_180.append(rotate_180(img))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my final list of images (X) in various orientations\n",
    "\n",
    "final_list_images= (\n",
    "    grayscale_image_array_only_350by350 + \n",
    "    images_reflected_horizontally + \n",
    "    images_reflected_vertically +\n",
    "    images_rotated_right +\n",
    "    images_rotated_left+\n",
    "    images_rotated_180\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the emotion labels (y) corresponding to the X images\n",
    "\n",
    "final_list_emotions= emotions_only_350by350*6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is done to get y and X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list grayscale_image_array_only_350by350 to array and reshaping the dimensions\n",
    "\n",
    "X = np.concatenate([\n",
    "    img.reshape(1,350,350,1)\n",
    "    for img in\n",
    "    final_list_images\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76938, 350, 350, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the text label into a number between 0 and 3 (because 4 labels) and converting this to \n",
    "# an array and reshaping\n",
    "\n",
    "label_to_number_map = {\n",
    "    label: idx for idx, label in enumerate(using_labels)}\n",
    "\n",
    "emo_numbers = [\n",
    "    label_to_number_map[emo] for emo in final_list_emotions\n",
    "]\n",
    "emo_numbers_array = np.array(emo_numbers).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the number into one hot encoding resulting in a matrix on size 12183 X 4\n",
    "\n",
    "y = keras.utils.to_categorical(\n",
    "    emo_numbers_array,\n",
    "    len(using_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a trained test split from the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 350, 350, 2)       20        \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 348, 348, 2)       38        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 174, 174, 2)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 174, 174, 4)       76        \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 172, 172, 4)       148       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 86, 86, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 86, 86, 8)         296       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 42, 42, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 42, 42, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 40, 40, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 20, 20, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 20, 20, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 18, 18, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 222,702\n",
      "Trainable params: 222,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(2, (3, 3), padding='same', input_shape=(350,350,1), activation=\"relu\"))\n",
    "model.add(Conv2D(2, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(4, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(4, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(8, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(8, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint('/Users/muhammadwaliji/Desktop/facial_expressions-master/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57703 samples, validate on 19235 samples\n",
      "Epoch 1/14\n",
      "57703/57703 [==============================] - 2818s 49ms/step - loss: 0.6108 - acc: 0.7531 - val_loss: 0.5888 - val_acc: 0.8198\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58882, saving model to /Users/muhammadwaliji/Desktop/facial_expressions-master/weights.hd5\n",
      "Epoch 2/14\n",
      "57703/57703 [==============================] - 2538s 44ms/step - loss: 0.4375 - acc: 0.8505 - val_loss: 0.4159 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58882 to 0.41590, saving model to /Users/muhammadwaliji/Desktop/facial_expressions-master/weights.hd5\n",
      "Epoch 3/14\n",
      "57703/57703 [==============================] - 2496s 43ms/step - loss: 0.3911 - acc: 0.8700 - val_loss: 0.3941 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41590 to 0.39411, saving model to /Users/muhammadwaliji/Desktop/facial_expressions-master/weights.hd5\n",
      "Epoch 4/14\n",
      "57703/57703 [==============================] - 2503s 43ms/step - loss: 0.3594 - acc: 0.8802 - val_loss: 0.3997 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.39411\n",
      "Epoch 5/14\n",
      "57703/57703 [==============================] - 2615s 45ms/step - loss: 0.3389 - acc: 0.8863 - val_loss: 0.3676 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.39411 to 0.36764, saving model to /Users/muhammadwaliji/Desktop/facial_expressions-master/weights.hd5\n",
      "Epoch 6/14\n",
      "57703/57703 [==============================] - 2497s 43ms/step - loss: 0.3189 - acc: 0.8939 - val_loss: 0.3606 - val_acc: 0.8783\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36764 to 0.36063, saving model to /Users/muhammadwaliji/Desktop/facial_expressions-master/weights.hd5\n",
      "Epoch 7/14\n",
      "57703/57703 [==============================] - 2503s 43ms/step - loss: 0.3006 - acc: 0.9010 - val_loss: 0.3726 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.36063\n",
      "Epoch 8/14\n",
      "57703/57703 [==============================] - 2501s 43ms/step - loss: 0.2849 - acc: 0.9054 - val_loss: 0.3666 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.36063\n",
      "Epoch 9/14\n",
      "57703/57703 [==============================] - 2508s 43ms/step - loss: 0.2674 - acc: 0.9115 - val_loss: 0.3720 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36063\n",
      "Epoch 10/14\n",
      "57703/57703 [==============================] - 2503s 43ms/step - loss: 0.2458 - acc: 0.9184 - val_loss: 0.3616 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.36063\n",
      "Epoch 11/14\n",
      "57703/57703 [==============================] - 2504s 43ms/step - loss: 0.2282 - acc: 0.9237 - val_loss: 0.3878 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.36063\n",
      "Epoch 12/14\n",
      "57703/57703 [==============================] - 2513s 44ms/step - loss: 0.2125 - acc: 0.9278 - val_loss: 0.3933 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.36063\n",
      "Epoch 13/14\n",
      "57703/57703 [==============================] - 2535s 44ms/step - loss: 0.1875 - acc: 0.9354 - val_loss: 0.4230 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.36063\n",
      "Epoch 14/14\n",
      "57703/57703 [==============================] - 2511s 44ms/step - loss: 0.1766 - acc: 0.9387 - val_loss: 0.4306 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efec827b8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=14,\n",
    "          validation_data=(X_test,y_test),\n",
    "          shuffle=True,\n",
    "          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9398"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHW Read it Back\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the json file that contains the model's structure\n",
    "f2 = Path(\"model_structure.json\")\n",
    "model_structure2 = f2.read_text()\n",
    "\n",
    "# Recreate the Keras model object from the json data\n",
    "model2 = model_from_json(model_structure2)\n",
    "\n",
    "# Re-load the model's trained weights\n",
    "model2.load_weights(\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[-0.05966711, -0.23504835]],\n",
       " \n",
       "         [[ 0.32434824, -0.09205551]],\n",
       " \n",
       "         [[ 0.1086131 ,  0.13341534]]],\n",
       " \n",
       " \n",
       "        [[[ 0.3417737 ,  0.17888854]],\n",
       " \n",
       "         [[-0.20828551, -0.23628773]],\n",
       " \n",
       "         [[-0.04358105,  0.31944153]]],\n",
       " \n",
       " \n",
       "        [[[-0.41921687,  0.43422508]],\n",
       " \n",
       "         [[ 0.44078884,  0.25204074]],\n",
       " \n",
       "         [[-0.18102227,  0.05787802]]]], dtype=float32),\n",
       " array([ 0.00759535, -0.00754651], dtype=float32)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the json file that contains the model's structure\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()\n",
    "\n",
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)\n",
    "\n",
    "# Re-load the model's trained weights\n",
    "model.load_weights(\"model_weights.h5\")\n",
    "\n",
    "# Load an image file to test, resizing it to 350 X 350 pixels (as required by this model)\n",
    "img = image.load_img(\"frog.png\", target_size=(350, 350))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "image_to_test = image.img_to_array(img)\n",
    "\n",
    "# Add a fourth dimension to the image (since Keras expects a list of images, not a single image)\n",
    "list_of_images = np.expand_dims(image_to_test, axis=0)\n",
    "\n",
    "# Make a prediction using the model\n",
    "results = model.predict(list_of_images)\n",
    "\n",
    "# Since we are only testing one image, we only need to check the first result\n",
    "single_result = results[0]\n",
    "\n",
    "# We will get a likelihood score for all 10 possible classes. Find out which class had the highest score.\n",
    "most_likely_class_index = int(np.argmax(single_result))\n",
    "class_likelihood = single_result[most_likely_class_index]\n",
    "\n",
    "# Get the name of the most likely class\n",
    "class_label = class_labels[most_likely_class_index]\n",
    "\n",
    "# Print the result\n",
    "print(\"This is image is a {} - Likelihood: {:2f}\".format(class_label, class_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
